{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0285fffe7caccef2cd4b23f4654a0f35",
     "grade": false,
     "grade_id": "cell-TITLE",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    " # EXAMEN GRANDS SYSTEMES - 4 GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "99f559868ca874874a860cb072a6d136",
     "grade": false,
     "grade_id": "cell-8929e79bf32acfdc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Importation des modules de calculs scientifiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5f68062021db27e1659c938f21334370",
     "grade": false,
     "grade_id": "cell-IMPORT",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as npl\n",
    "import scipy as sp\n",
    "import scipy.sparse as sps\n",
    "import scipy.optimize as spo\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a11e4c5c7c204c74772b4d347de046bf",
     "grade": false,
     "grade_id": "cell-7ec7c6f131552d5e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Instructions pour cet examen\n",
    "\n",
    "- Les documents et les objets connectés sont strictement interdits. \n",
    "- Sauf contre-indications, il ne faut pas utiliser de fonctions des bibliothèques Python lorsque votre propre implémentation est demandée. \n",
    "- Lors de la correction, vos fonctions seront testées avec différents arguments, il est donc **primordial** de respecter la syntaxe et le format des fonctions.\n",
    "- Les matrices et vecteurs sont pris au format `array` de numpy/scipy. De plus, les matrices peuvent être au format sparse.\n",
    "- Vous êtes libre de définir des fonctions autres que celles que l'on vous demande explicitement afin de rendre vos codes plus simples ou plus lisibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "93067ee750aab09d86d4c6f9e57872bb",
     "grade": false,
     "grade_id": "cell-b1aa02adfeee2709",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## I. Calcul d'éléments propres par la minimisation de Shmoukly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "19f8f66cd2ecabf28f10025865366311",
     "grade": false,
     "grade_id": "cell-67190fc61430c570",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Soit $A\\in M_n(\\mathbb{R})$ une matrice symétrique définie positive. Pour tout $m\\in\\mathbb{R}$, on considère à présent la fonctionnelle $J_m$ donnée, $\\forall x\\in\\mathbb{R}^n$, par\n",
    "\n",
    "$$\n",
    "J_m(x) = \\frac12\\langle x,Ax\\rangle - m \\|x\\|_2.\n",
    "$$\n",
    "\n",
    "On remarque que le gradient de $J$ se calcule comme\n",
    "\n",
    "$$\n",
    "\\nabla J_m(x) = Ax - m \\frac{x}{\\|x\\|_2}.\n",
    "$$\n",
    "\n",
    "Le but de cette partie est de minimiser cette fonctionnelle. En effet, l'unique minimum de $J$ vérifie les conditions d'optimalité du premier ordre, c'est-à-dire\n",
    "\n",
    "$$\n",
    "Ax - m \\frac{x}{\\|x\\|_2} = 0 \\quad \\Leftrightarrow \\quad Ax = \\frac{m}{\\|x\\|_2} x,\n",
    "$$\n",
    "\n",
    "ce qui nous donne donc l'élément propre $(x,m/\\|x\\|_2)$. Il reste à voir ce que cela donne en pratique...\n",
    "\n",
    "Afin de calculer le minimum de cette fonction, nous allons utiliser des méthodes de type gradient. C'est-à-dire que l'on va construire une suite $(x_k)_{k\\geq 0}$ à partir d'une relation itérative du type\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k + \\alpha_k p_k,\n",
    "$$\n",
    "\n",
    "où $p_k$ est une direction de descente et $\\alpha_k$ un scalaire correspondant au pas de la méthode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b67efee1225ae5528a357a3cb44f5870",
     "grade": false,
     "grade_id": "cell-a61b46e481d38e05",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1) Algorithme de gradient classique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4c51f90cf24a02ce085ee8b63224ff60",
     "grade": false,
     "grade_id": "cell-afa01a41a99f9dc0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "On commence par un algorithme de gradient classique prenant $p_k = -\\nabla J_m(x_k)$ et $\\alpha_k = h\\in\\mathbb{R}$.\n",
    "L'algorithme prend la forme suivante\n",
    "\n",
    "$$\n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\textrm{Choisir }x_0\\in\\mathbb{R}^n\n",
    "\\\\ \\textrm{Pour }k = 0,1,2,\\ldots,\\textrm{itermax}:\n",
    "\\\\ \\hspace{2em} p_k = -\\nabla J_m(x_k)\n",
    "\\\\ \\hspace{2em} x_{k+1} = x_k + h p_k\n",
    "\\\\ \\hspace{2em} \\textrm{Si }\\|h p_k\\|_2 < \\varepsilon \\|x_k\\|_2 :\n",
    "\\\\ \\hspace{4em} \\textrm{Fin de la boucle sur }k\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    ">**A faire** : Implémenter la fonction `Gradient_Cla_J` qui met en oeuvre l'algorithme précédent. Elle prendra en arguments d'entrée:\n",
    "- une matrice carré `A` de taille $n$,\n",
    "- un scalaire `m` correspondant à $m$,\n",
    "- un scalaire `h` correspondant au pas $h$,\n",
    "- un vecteur initial `x_0` de taille $n$,\n",
    "- une tolérance `epsilon` (optionel à `1e-15`),\n",
    "- un nombre maximal d'itérations `itermax` (optionel à `1e4`),\n",
    "\n",
    ">et donnera en sortie:\n",
    "- le vecteur `x` solution de la minimisation de la fonction $J_m$,\n",
    "- le nombre d'itérations réalisé par la méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "6b67986a8cc8d6301087038e27822daf",
     "grade": true,
     "grade_id": "cell-80995a52e3256f78",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# VOTRE CODE ICI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8212ac505c9e63fdd39f3c86696a4583",
     "grade": false,
     "grade_id": "cell-8fa11ec653b7f5e2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    ">**A faire** : Tester votre fonction avec le script ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "45ac9e1c5067dffe05d8ab71c408d944",
     "grade": false,
     "grade_id": "cell-ea57713e85a3ad3f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "m = 1\n",
    "h = 0.5\n",
    "x_0 = np.random.random(3)\n",
    "A = np.array([[1.,1.,0.],[1.,2.,0.],[0.,0.,3.]])\n",
    "x,k = Gradient_Cla_J(A,m,h,x_0)\n",
    "print(\"Nombre d'itérations: \",k)\n",
    "print(\"Quotient de Rayleigh de x: \",np.dot(np.dot(A,x),x)/npl.norm(x)**2)\n",
    "print(\"Valeurs propres de A: \",npl.eig(A)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "af7a0a99332001a9c1591debf3e2038f",
     "grade": false,
     "grade_id": "cell-817aeab55e364e91",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Question 1**: A priori, vers quelle élément propre converge la méthode?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8cf8a229e109846f9cafb19643e7a118",
     "grade": true,
     "grade_id": "cell-153d2cee05d4d610",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Votre réponse ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bdb8c0734e5d2393c1308ba31838e438",
     "grade": false,
     "grade_id": "cell-7beb912139da9f78",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    ">**A faire** : Tester votre fonction avec le script ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d0924acc2e09fffd4c0ec7bf5ad85564",
     "grade": false,
     "grade_id": "cell-8881d6bc2e822cc5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "L = np.zeros(100)\n",
    "I = np.zeros(100)\n",
    "h = 0.5\n",
    "x_0 = np.random.random(3)\n",
    "A = np.array([[1.,1.,0.],[1.,2.,0.],[0.,0.,3.]])\n",
    "for k in range(100):\n",
    "    x,niter = Gradient_Cla_J(A,k+1,h,x_0)\n",
    "    I[k] = niter\n",
    "    L[k] = np.dot(np.dot(A,x),x)/npl.norm(x)**2\n",
    "plt.plot(L)\n",
    "plt.show()\n",
    "plt.plot(I)\n",
    "print(\"Nombre d'itérations: \",k)\n",
    "print(\"Quotient de Rayleigh de x: \",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a7303b249ed7faa22a00cc5e34782744",
     "grade": false,
     "grade_id": "cell-a0cdda405cb62005",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## 2) Algorithme de gradient de plus profonde descente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "25465683e7c6e9bb6f2f3e1e5dc87eda",
     "grade": false,
     "grade_id": "cell-baec11ebeb7d7b70",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "On passe maintenant à un algorithme de gradient de la plus profonde descente en prenant $p_k = -\\nabla J_m(x_k)$ et $\\alpha_k$ solution du problème de minimisation\n",
    "\n",
    "$$\n",
    "\\alpha_k = \\textrm{argmin}_{\\alpha\\in\\mathbb{R}} J_m(x_k + \\alpha p_k).\n",
    "$$\n",
    "\n",
    "Dans ce cas, il n'y a malheureusement pas d'expression explicite pour $\\alpha_k$ et on utilisera la fonction de minimisation `spo.minimize` pour l'obtenir. Cette fonction prend en argument d'entrée:\n",
    "- une fonction `f` dont on cherche le minimum,\n",
    "- un point `x_0` initial.\n",
    "\n",
    "Le script ci-dessous permet de comprendre le fonctionnement de l'argument de sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c66abaee9e2cc2f1e4d697ef831cd6fd",
     "grade": false,
     "grade_id": "cell-a624034069683ca5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def g(z,k):\n",
    "    return k*z**2 - z + 3\n",
    "f = lambda z: g(z,1)\n",
    "min = spo.minimize_scalar(f).x\n",
    "print(min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "45a8d74e16a181ee839ea09db1222ace",
     "grade": false,
     "grade_id": "cell-8480441d64f544e0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Au final, l'algorithme s'écrit donc\n",
    "$$\n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\textrm{Choisir }x_0\\in\\mathbb{R}^n\n",
    "\\\\ \\textrm{Pour }k = 0,1,2,\\ldots,\\textrm{itermax}:\n",
    "\\\\ \\hspace{2em} p_k = -\\nabla J_m(x_k)\n",
    "\\\\ \\hspace{2em} \\textrm{Si }\\|p_k\\|_2 < \\varepsilon :\n",
    "\\\\ \\hspace{4em} \\textrm{Fin de la boucle sur }k\n",
    "\\\\ \\hspace{2em} \\alpha_k = \\min_{\\alpha\\in\\mathbb{R}} J_m(x_k + \\alpha p_k)\n",
    "\\\\ \\hspace{2em} x_{k+1} = x_k + \\alpha_k p_k\n",
    "\\\\ \\hspace{2em} \\textrm{Si }\\|\\alpha_k p_k\\|_2 < \\varepsilon \\|x_k\\|_2 :\n",
    "\\\\ \\hspace{4em} \\textrm{Fin de la boucle sur }k\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    ">**A faire** : Implémenter la fonction `Gradient_Opt_J` qui met en oeuvre l'algorithme précédent. Elle prendra en arguments d'entrée:\n",
    "- une matrice carré `A` de taille $n$,\n",
    "- un scalaire `m` correspondant à $m$,\n",
    "- un vecteur initial `x_0` de taille $n$,\n",
    "- une tolérance `epsilon` (optionel à `1e-15`),\n",
    "- un nombre maximal d'itérations `itermax` (optionel à `1e4`),\n",
    "\n",
    ">et donnera en sortie:\n",
    "- le vecteur `x` solution de la minimisation de la fonction $J_m$,\n",
    "- le nombre d'itérations réalisé par la méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "8e624c5dd3f63fa4062b76a51f82c312",
     "grade": true,
     "grade_id": "cell-f18ebdabd759dcc3",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# VOTRE CODE ICI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bc5044405f06008bc98c6bfa8df6a8c2",
     "grade": false,
     "grade_id": "cell-64ff97e830c41afa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    ">**A faire** : Tester votre fonction avec le script ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "51d7dfb27b1b6dde0d1d7ac32d4d8d6f",
     "grade": false,
     "grade_id": "cell-c664775b9f82cd42",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "m = 1\n",
    "x_0 = np.random.random(3)\n",
    "A = np.array([[1.,1.,0.],[1.,2.,0.],[0.,0.,3.]])\n",
    "x,k = Gradient_Opt_J(A,m,x_0)\n",
    "print(\"Nombre d'itérations: \",k)\n",
    "print(\"Quotient de Rayleigh de x: \",np.dot(np.dot(A,x),x)/npl.norm(x)**2)\n",
    "print(\"Valeurs propres de A: \",npl.eig(A)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9841976dd610ec33c2b65a36d1616d18",
     "grade": false,
     "grade_id": "cell-448ada55ed61d202",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Question 2**: D'après vos tests, quelle est la méthode la plus rapide, en terme d'itérations, entre la méthode du gradient classique et celui de plus profonde descente?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "499aff98e0674520000514f6123f1b91",
     "grade": true,
     "grade_id": "cell-fb35d141b1e438e7",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Votre réponse ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bdc61373940090f77ecafb5a88d5737e",
     "grade": false,
     "grade_id": "cell-c315c513bb07a8ff",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## II. Calcul d'éléments propres par la minimisation du quotient de Rayleigh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5e6b109f51a73161da6b6e8f19fcfe82",
     "grade": false,
     "grade_id": "cell-dbd866a791501ff1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Le but de cette partie est de reprendre les méthodes précédentes mais en les appliquant à la minimisation du quotient de Rayleigh. On rappelle que, pour une matrice $A$ carrée de taille $n\\in\\mathbb{N}$, le quotient de Rayleigh est définit, $\\forall x\\in\\mathbb{R}^n$, comme\n",
    "\n",
    "$$\n",
    "r_A(x) = \\frac{\\langle x, Ax\\rangle}{\\|x\\|^2_2}.\n",
    "$$\n",
    "\n",
    "Lorsque la matrice $A$ est diagonalisable et que l'on considère un élément propre $(u,\\lambda)$, où $u$ est un vecteur propre associé à la valeur propre $\\lambda$, on a en particulier que\n",
    "\n",
    "$$\n",
    "r_A(u) = \\lambda.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b4e927924826c50b4b2b3b26ab5ab3d8",
     "grade": false,
     "grade_id": "cell-91ce06712791e8ec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Question 1**: Pour une matrice $A$ symétrique, selon vous, quelle est la solution du problème de minimisation suivant\n",
    "\n",
    "$$\n",
    "\\min_{x\\in\\mathbb{R}^n, x\\neq 0} r_A(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ccad6c5ff89c97e235a42a532c17ad6e",
     "grade": true,
     "grade_id": "cell-325183cc599779dc",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Votre réponse ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e2918c6ba15c88d08a8631d14e9e986e",
     "grade": false,
     "grade_id": "cell-d81c2f0870352bca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Comme pour la partie précédente, on va s'orienter vers des méthodes de type gradient afin de résoudre ce problème de minimisation. L'approche générique est de considérer une suite $(x_k)_{k\\geq 0}$ construite à partir d'une relation itérative du type\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k + \\alpha_k p_k,\n",
    "$$\n",
    "\n",
    "où $p_k$ est une direction de descente et $\\alpha_k$ un scalaire permettant d'assurer une descente optimale. Dans cette partie, nous n'utiliserons pas la fonction `spo.minimize` puisque la recherche du $\\alpha_k$ optimal s'écrit simplement comme la recherche de la racine positive du polynôme suivant\n",
    "\n",
    "$$\n",
    "Q_k(\\alpha) = a_k\\alpha^2 + b_k \\alpha + c_k,\n",
    "$$\n",
    "avec\n",
    "$$\n",
    "\\left\\{\\begin{array}{ll}\n",
    "a_k = \\langle p_k\\,,\\,Ap_k\\rangle\\langle p_k\\,,\\,x_k\\rangle - \\langle x_k\\,,\\,Ap_k\\rangle\\langle p_k\\,,\\,p_k\\rangle\n",
    "\\\\ b_k = \\langle x_k\\,,\\,Ax_k\\rangle\\langle p_k\\,,\\,p_k\\rangle - \\langle p_k\\,,\\,Ap_k\\rangle\\langle x_k\\,,\\,x_k\\rangle\n",
    "\\\\ c_k = \\langle x_k\\,,\\,Ap_k\\rangle\\langle x_k\\,,\\,x_k\\rangle - \\langle x_k\\,,\\,Ax_k\\rangle\\langle x_k\\,,\\,p_k\\rangle\n",
    "\\end{array}\\right. .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b3d32993bf03d3b01b6831353e8c28fe",
     "grade": false,
     "grade_id": "cell-5cda2a0e921ac4ea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 1) Algorithme de gradient "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9d5a3f3c64387a00574220ebb7c53304",
     "grade": false,
     "grade_id": "cell-025e52760b7b3f8b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Le gradient du quotient de Rayleigh est donné par\n",
    "\n",
    "$$\n",
    "\\nabla r_A(x) = 2\\frac{Ax - r_A(x)x}{\\|x\\|_2^2}.\n",
    "$$\n",
    "\n",
    "On commence par un algorithme de gradient de la plus profonde descente en prenant $p_k = -\\nabla r_A(x_k)$.\n",
    "L'algorithme prend alors la forme suivante\n",
    "\n",
    "$$\n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\textrm{Choisir }x_0\\in\\mathbb{R}^n\n",
    "\\\\ \\textrm{Pour }k = 0,1,2,\\ldots,\\textrm{itermax}:\n",
    "\\\\ \\hspace{2em} p_k = -\\nabla r_A(x_k)\n",
    "\\\\ \\hspace{2em} \\textrm{Si }\\|p_k\\|_2 < \\varepsilon :\n",
    "\\\\ \\hspace{4em} \\textrm{Fin de la boucle sur }k\n",
    "\\\\ \\hspace{2em} \\alpha_k = \\min_{\\alpha\\in\\mathbb{R}} r_A(x_k + \\alpha p_k)\n",
    "\\\\ \\hspace{2em} x_{k+1} = x_k + \\alpha_k p_k\n",
    "\\\\ \\hspace{2em} \\textrm{Si }\\|\\alpha_k p_k\\|_2 < \\varepsilon \\|x_k\\|_2 :\n",
    "\\\\ \\hspace{4em} \\textrm{Fin de la boucle sur }k\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    ">**A faire** : Implémenter la fonction `Gradient_Opt_R` qui met en oeuvre l'algorithme précédent. Elle prendra en arguments d'entrée:\n",
    "- une matrice carré `A` de taille $n$,\n",
    "- un vecteur initial `x_0` de taille $n$,\n",
    "- une tolérance `epsilon` (optionel à `1e-15`),\n",
    "- un nombre maximal d'itérations `itermax` (optionel à `1e4`),\n",
    "\n",
    ">et donnera en sortie:\n",
    "- le vecteur `x` solution de la minimisation du quotient de Rayleigh,\n",
    "- le nombre d'itérations réalisé par la méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "88e98dae1711e698f7b4d7a3eeba21ed",
     "grade": false,
     "grade_id": "cell-1a58ad52c9c95f9f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# VOTRE CODE ICI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1a59738d7d9b4cdcb36f7846c07e88ab",
     "grade": false,
     "grade_id": "cell-a637dd4654196d52",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    ">**A faire** : Tester votre fonction avec le script ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8f69b20f203e4672d53fd3b8ce3ff196",
     "grade": false,
     "grade_id": "cell-e4222f0d10ffce73",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x_0 = np.array([3.,-1.,1.])\n",
    "A = np.array([[1.,1.,0.],[1.,2.,0.],[0.,0.,3.]])\n",
    "x,k = Gradient_Opt_R(A,x_0)\n",
    "print(\"Nombre d'itérations: \",k)\n",
    "print(\"Quotient de Rayleigh de x: \",np.dot(np.dot(A,x),x)/npl.norm(x)**2)\n",
    "print(\"Valeurs propres de A: \",npl.eig(A)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d58cec3cdf1174c727f6560d5da602a1",
     "grade": false,
     "grade_id": "cell-5aed324a2a018ccd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Question 2**: Est-ce que la méthode donne le bon résultat?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "38381b406cad4518b8b69a1f4939c1cd",
     "grade": true,
     "grade_id": "cell-31114ab57ef76c63",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Votre réponse ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b10cb85b12944fe79e242a4f2a1ddf21",
     "grade": false,
     "grade_id": "cell-509e61d2ac01e5a2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    ">**A faire** : Tester votre fonction avec le script ci-dessous. Ce script donne la matrice du laplacien pour une discrétisation par différences finies d'ordre 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Laplacian(n) :\n",
    "    return (n+1)**2*sps.diags([2*np.ones(n),-1*np.ones(n-1),-1*np.ones(n-1)], [0, -1, 1]).tocsc()\n",
    "def Eig_Lapl(n):\n",
    "    return np.array(4*((n+1)*np.sin(np.arange(1,n+1)*np.pi/(2*(n+1))))**2)\n",
    "\n",
    "N = 200\n",
    "A = Laplacian(N)\n",
    "x_0 = np.ones((N,1))\n",
    "x,k = Gradient_Opt_R(A,x_0)\n",
    "print(\"Nombre d'itérations: \",k)\n",
    "print(\"Quotient de Rayleigh de x: \",(A.dot(x).T.dot(x)/npl.norm(x)**2)[0][0])\n",
    "print(\"Première valeur propre de A: \",Eig_Lapl(N)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0f155af98036f7d30f2f42ed1c25dbcd",
     "grade": false,
     "grade_id": "cell-f5166d62387371b7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Question 3**: Selon vous, la précision obtenue est-elle en adéquation avec la tolérance de l'algorithme?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8fdbb0866002223837ae1405c246d2a7",
     "grade": true,
     "grade_id": "cell-af2bfda48649e916",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Votre réponse ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "348f2dd5301eb132b6ba656df373e26d",
     "grade": false,
     "grade_id": "cell-93d9bdc94a62852e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 2) Algorithme du gradient conjugué"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fdb976bd1064666b701f34349f6b3893",
     "grade": false,
     "grade_id": "cell-c71477ec4f89864a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Nous allons à présent nous tourner vers un algorithme plus sophistiqué: l'algorithme du gradient conjugé non-linéaire. La direction de descente $p_k$ est alors donnée par une famille $A$-conjuguée. Plus précisément, on prend\n",
    "$$\n",
    "p_k = g_k + \\beta_{k-1} p_{k-1},\n",
    "$$\n",
    "où $g_k = - \\nabla r_A(x)$ et le paramètre $\\beta_{k-1}$ est donné par\n",
    "$$\n",
    "\\beta_{k-1} = \\frac{\\langle g_{k}\\,,\\, g_k-g_{k-1} \\rangle}{\\langle g_{k-1}\\,,\\, g_{k-1} \\rangle}.\n",
    "$$\n",
    "Ceci nous amène donc à la forme suivante de l'algorithme\n",
    "L'algorithme prend alors la forme suivante\n",
    "\n",
    "$$\n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\textrm{Choisir }x_0\\in\\mathbb{R}^n\n",
    "\\\\ \\textrm{Poser }g_{-1} = -\\nabla r_A(x_0)\\quad\\textrm{et}\\quad p_{-1} = 0\n",
    "\\\\ \\textrm{Pour }k = 0,1,2,\\ldots,\\textrm{itermax}:\n",
    "\\\\ \\hspace{2em} g_k = -\\nabla r_A(x_k)\n",
    "\\\\ \\hspace{2em} \\beta_{k-1} =  \\frac{\\langle g_{k}\\,,\\, g_k - g_{k-1}\\, \\rangle}{\\langle g_{k-1}\\,,\\, g_{k-1}\\, \\rangle}\n",
    "\\\\ \\hspace{2em} p_k =  g_k + \\beta_{k-1} p_{k-1}\n",
    "\\\\ \\hspace{2em} \\textrm{Si }\\|p_k\\|_2 < \\varepsilon:\n",
    "\\\\ \\hspace{4em} \\textrm{Fin de la boucle sur }k\n",
    "\\\\ \\hspace{2em} \\alpha_k = \\min_{\\alpha\\in\\mathbb{R}} r_A(x_k + \\alpha p_k)\n",
    "\\\\ \\hspace{2em} x_{k+1} = x_k + \\alpha_k p_k\n",
    "\\\\ \\hspace{2em} \\textrm{Si }\\|\\alpha_k p_k\\|_2 < \\varepsilon \\|x_k\\|_2 :\n",
    "\\\\ \\hspace{4em} \\textrm{Fin de la boucle sur }k\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    ">**A faire** : Implémenter la fonction `Gradient_Conj_R` qui met en oeuvre l'algorithme précédent. Elle prendra en arguments d'entrée:\n",
    "- une matrice carré `A` de taille $n$,\n",
    "- un vecteur initial `x_0` de taille $n$,\n",
    "- une tolérance `epsilon` (optionel à `1e-15`),\n",
    "- un nombre maximal d'itérations `itermax` (optionel à `1e4`),\n",
    "\n",
    ">et donnera en sortie:\n",
    "- le vecteur `x` solution de la minimisation du quotient de Rayleigh,\n",
    "- le nombre d'itérations réalisé par la méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "4c0930ccc0e9b22679f8440c0bc61ad7",
     "grade": false,
     "grade_id": "cell-b666efda0b855f04",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# VOTRE CODE ICI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "48bc501b0179a7608f7ac8f16736a402",
     "grade": false,
     "grade_id": "cell-85fffd392265e685",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    ">**A faire** : Tester votre fonction avec le script ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c850201f685effd41ada601648c7f753",
     "grade": false,
     "grade_id": "cell-600cda5dc2191b64",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "A = Laplacian(N)\n",
    "x_0 = np.ones((N,1))\n",
    "x,k = Gradient_Conj_R(A,x_0)\n",
    "print(\"Nombre d'itérations: \",k)\n",
    "print(\"Quotient de Rayleigh de x: \",(A.dot(x).T.dot(x)/npl.norm(x)**2)[0][0])\n",
    "print(\"Première valeur propre de A: \",Eig_Lapl(N)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "764e4228a10c84f8a1a9475a1af5061d",
     "grade": false,
     "grade_id": "cell-fc9ccefe18b5fd9f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "**Question 4**: Selon vous, la précision obtenue est-elle en adéquation avec la tolérance de l'algorithme?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "6bb1fdead38c444f74e89ddb9de32747",
     "grade": true,
     "grade_id": "cell-5c00ebd78674b526",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Votre réponse ici"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
